{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"DeepTMHMM.partitions.json\")\n",
    "folds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv0', 'cv1', 'cv2', 'cv3', 'cv4'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds.keys() #ok, so these are the folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prototyping models, I would pick one fixed assignment (e.g. train 0/1/2, val 3, test 4)\n",
    "\n",
    "-> let's create a training and test partitioning instead of the folds \n",
    "\n",
    "Structure is dict -> list of dicts with protein name as key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv0\n",
      "cv1\n",
      "cv2\n",
      "cv3\n",
      "cv4\n"
     ]
    }
   ],
   "source": [
    "for key in folds:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q5VT06', 'P29994', 'Q14315', 'Q9U943', 'Q9VDW6', 'Q14789', 'Q8WXX0', 'P69332', 'P36022', 'P04875', 'Q01484', 'Q05470', 'Q96Q15', 'O83774', 'Q5I6C7', 'Q96T58', 'Q9UKN1', 'Q9SMH5', 'P14217', 'P0DTC2', 'Q3KNY0', 'Q8IZQ1', 'Q9VKA4', 'Q9VC56', 'Q7TMY8', 'Q868Z9', 'Q9P2D1', 'Q6KC79', 'F8VPN2', 'P98161', 'O83276', 'Q61001']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "with open(\"missing_samples/missing_prots.txt\") as f:\n",
    "    exclude_list = f.readlines()\n",
    "\n",
    "exclude_list = [x.replace(\"\\n\",\"\") for x in exclude_list]\n",
    "\n",
    "with open(\"data_quality_control.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "mismatches = data[\"Different sequence\"]\n",
    "for mismatch in mismatches:\n",
    "    exclude_list.append(mismatch)\n",
    "\n",
    "split_wise_removed = {\"train\":0,\"val\":0,\"test\":0}\n",
    "\n",
    "for fold in list(folds.keys()):\n",
    "    for sample in folds[fold]:#list \n",
    "        #check if alfafold download succeeded; if not, exclude from set. Save number of discarded samples in dict \n",
    "        if(sample[\"id\"] in exclude_list):\n",
    "            if(fold==\"cv1\" or fold==\"cv2\" or fold==\"cv3\"):\n",
    "                split_wise_removed[\"train\"] += 1\n",
    "            elif(fold==\"cv3\"):\n",
    "                split_wise_removed[\"val\"] += 1\n",
    "            else:\n",
    "                split_wise_removed[\"test\"] += 1\n",
    "\n",
    "        else:\n",
    "            if(fold!=\"cv4\" and fold!=\"cv3\"):\n",
    "                train.append(sample)\n",
    "            if(fold==\"cv3\"):\n",
    "                val.append(sample)\n",
    "            elif(fold==\"cv4\"):\n",
    "                test.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"splits/prototype/train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train,f)\n",
    "\n",
    "with open(\"splits/prototype/val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val,f)\n",
    "\n",
    "with open(\"splits/prototype/test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test,f)\n",
    "\n",
    "with open(\"splits/prototype/missing_distribution.pkl\", \"wb\") as f:\n",
    "    pickle.dump(split_wise_removed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 25, 'val': 0, 'test': 7}\n"
     ]
    }
   ],
   "source": [
    "print(split_wise_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129 704 711\n"
     ]
    }
   ],
   "source": [
    "print(len(train),len(val),len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that there is no overlap in proteins for any of the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def assure_difference(train_li,val_li,test_li):\n",
    "    \"\"\"Checks that splits are not contaminated in any sort of way\n",
    "        Returns bool. True = good\n",
    "    \"\"\"\n",
    "    train_prots = [x[\"id\"] for x in train]\n",
    "    val_prots = [x[\"id\"] for x in val]\n",
    "    test_prots = [x[\"id\"] for x in test]\n",
    "\n",
    "    ###check that there are no repetitions\n",
    "    rep_check = (len(set(train_prots))==len(train_prots) and len(set(val_prots))==len(val_prots) and len(set(test_prots))==len(test_prots))\n",
    "\n",
    "    #check that no sets have an intersection\n",
    "    s_train = set(train_prots)\n",
    "    s_val = set(val_prots)\n",
    "    s_test = set(test_prots)\n",
    "\n",
    "    overlap_check = (len(s_train & s_val)==0 and len(s_train & s_test) == 0 and len(s_val & s_test) == 0)\n",
    "    \n",
    "    return (overlap_check and rep_check and overlap_check!=False)\n",
    "\n",
    "print(assure_difference(train,val,test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's implement a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'P10384',\n",
       " 'sequence': 'MSQKTLFTKSALAVAVALISTQAWSAGFQLNEFSSSGLGRAYSGEGAIADDAGNVSRNPALITMFDRPTFSAGAVYIDPDVNISGTSPSGRSLKADNIAPTAWVPNMHFVAPINDQFGWGASITSNYGLATEFNDTYAGGSVGGTTDLETMNLNLSGAYRLNNAWSFGLGFNAVYARAKIERFAGDLGQLVAGQIMQSPAGQTQQGQALAATANGIDSNTKIAHLNGNQWGFGWNAGILYELDKNNRYALTYRSEVKIDFKGNYSSDLNRAFNNYGLPIPTATGGATQSGYLTLNLPEMWEVSGYNRVDPQWAIHYSLAYTSWSQFQQLKATSTSGDTLFQKHEGFKDAYRIALGTTYYYDDNWTFRTGIAFDDSPVPAQNRSISIPDQDRFWLSAGTTYAFNKDASVDVGVSYMHGQSVKINEGPYQFESEGKAWLFGTNFNYAF',\n",
       " 'labels': 'SSSSSSSSSSSSSSSSSSSSSSSSSPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBPPPPPPPPBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBPPPPPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBPPPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBBPPPPBBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBBPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOBBBBBBBBBPPPPBBBBBBBBOOOOOOOOOOOOOOOOOOOOOOOOBBBBBBBPP'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pdbreader\n",
    "import pickle \n",
    "\n",
    "\n",
    "with open(\"splits/prototype/train.pkl\",'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(\"splits/prototype/val.pkl\",'rb') as f:\n",
    "    val = pickle.load(f)\n",
    "\n",
    "with open(\"splits/prototype/test.pkl\",'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transmembraneDataset(Dataset):\n",
    "    def __init__(self,data_li,path):\n",
    "        self.protein_names = [x[\"id\"] for x in data_li]\n",
    "        self.residue_sequnces = [x[\"sequence\"] for x in data_li]\n",
    "        self.label_sequences = [x[\"labels\"] for x in data_li]        \n",
    "        self.pdb_path = path\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        name = self.protein_names[idx]\n",
    "        pdb_file_path = self.pdb_path + name + \".pdb\" #todo: read graph-tensor instead\n",
    "        pdb_file = pdbreader.read_pdb(pdb_file_path)\n",
    "        label = self.label_sequences[idx]\n",
    "        return name, pdb_file_path, label\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(transmembraneDataset(train,\"data/graphein_downloads/train/\"),batch_size=1,shuffle=True)\n",
    "val_dataloader = DataLoader(transmembraneDataset(val,\"data/graphein_downloads/train/\"),batch_size=1,shuffle=True)\n",
    "test_dataloader = DataLoader(transmembraneDataset(test,\"data/graphein_downloads/train/\"),batch_size=1,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that loading pdb works for all loaders (ie. no errors thrown at any point through this block)\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    _, pdb_file_path, _ = data\n",
    "    _ = pdb_file = pdbreader.read_pdb(pdb_file_path[0])\n",
    "\n",
    "\n",
    "for i, data in enumerate(val_dataloader):\n",
    "    _, pdb_file_path, _ = data\n",
    "    _ = pdb_file = pdbreader.read_pdb(pdb_file_path[0])\n",
    "    \n",
    "for i, data in enumerate(test_dataloader):\n",
    "    _, pdb_file_path, _ = data\n",
    "    _ = pdb_file = pdbreader.read_pdb(pdb_file_path[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
